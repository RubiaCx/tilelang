{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1) 环境初始化与路径/版本打印"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TileLang version: 0.1.5\n",
            "Using tilelang from: /home/chenxi/miniconda3/envs/tilelang/lib/python3.12/site-packages/tilelang/__init__.py\n",
            "A: /home/chenxi/tilelang/examples/flash_attention/example_mha_fwd_bshd.py\n",
            "B: /home/chenxi/tilelang/examples/flash_attention/example_mha_fwd_bshd_wgmma_pipelined-2.py\n"
          ]
        }
      ],
      "source": [
        "# Inspect TileLang Pipelined IR: default vs explicit order/stage/group\n",
        "import os, sys, importlib.util, textwrap, re\n",
        "from pathlib import Path\n",
        "import tilelang\n",
        "from tilelang.jit.kernel import JITKernel\n",
        "\n",
        "ROOT = Path('/home/chenxi')\n",
        "A_PATH = ROOT / 'tilelang/examples/flash_attention/example_mha_fwd_bshd.py'\n",
        "B_PATH = ROOT / 'tilelang/examples/flash_attention/example_mha_fwd_bshd_wgmma_pipelined-2.py'\n",
        "OUT_DIR = ROOT / 'tilelang/notebooks/artifacts'\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('TileLang version:', tilelang.__version__)\n",
        "print('Using tilelang from:', tilelang.__file__)\n",
        "print('A:', A_PATH)\n",
        "print('B:', B_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2) 用 `importlib` 的 `spec_from_file_location` 安全加载两个示例模块，并打印加载的模块名"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Joint配置数量: 6\n",
            "Tile配置数量: 4 (固定stage=1)\n",
            "Loaded modules: example_mha_fwd_bshd example_mha_fwd_bshd_wgmma_pipelined_2\n"
          ]
        }
      ],
      "source": [
        "# Helpers to safely import example modules without triggering autotune side-effects\n",
        "\n",
        "def load_module(path: str, mod_name: str):\n",
        "    spec = importlib.util.spec_from_file_location(mod_name, path)\n",
        "    mod = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(mod)\n",
        "    return mod\n",
        "\n",
        "A = load_module(str(A_PATH), 'example_mha_fwd_bshd')\n",
        "B = load_module(str(B_PATH), 'example_mha_fwd_bshd_wgmma_pipelined_2')\n",
        "\n",
        "print('Loaded modules:', A.__name__, B.__name__)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3) 分别调用两份脚本的 `flashattn(...)` 得到 `JITKernel`，从中取 `prim_func`，即编译前的 TIR ，保存，用于直观看到 `T.Pipelined(...)` 的源级调用位置与上下文"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-09 16:36:24,465 WARNING:Tunable parameters ['block_M', 'block_N', 'num_stages', 'threads'] already provided during auto-tuning. Skipping compilation and using direct JIT\n",
            "2025-09-09 16:36:24  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `kernel_impl` with `out_idx=[3]`\n",
            "2025-09-09 16:36:35  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `kernel_impl`\n",
            "2025-09-09 16:36:35,130 WARNING:Tunable parameters ['block_M', 'block_N', 'num_stages', 'threads'] already provided during auto-tuning. Skipping compilation and using direct JIT\n",
            "2025-09-09 16:36:35  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[3]`\n",
            "2025-09-09 16:36:45  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
            "A kernel ok: True\n",
            "B kernel ok: True\n",
            "A prim type: <class 'tvm.tir.function.PrimFunc'>\n",
            "B prim type: <class 'tvm.tir.function.PrimFunc'>\n",
            "Saved pre-lower scripts to /home/chenxi/tilelang/notebooks/artifacts\n"
          ]
        }
      ],
      "source": [
        "# Build kernels via JIT (avoid manual lower), then extract PrimFuncs\n",
        "from tilelang.jit.kernel import JITKernel\n",
        "\n",
        "# A: use flashattn (decorated), returns JITKernel; autotune will be bypassed if all tunables provided\n",
        "_tmp_A = A.flashattn(batch=1, heads=1, seq_len=256, dim=64, is_causal=False,\n",
        "                     block_M=64, block_N=64, num_stages=2, threads=128)\n",
        "# B: explicit pipelined version\n",
        "_tmp_B = B.flashattn(batch=1, heads=1, seq_len=256, dim=64, is_causal=False,\n",
        "                     block_M=64, block_N=64, num_stages=2, threads=128)\n",
        "\n",
        "assert isinstance(_tmp_A, JITKernel), f\"A should be JITKernel, got {_tmp_A}\"\n",
        "assert isinstance(_tmp_B, JITKernel), f\"B should be JITKernel, got {_tmp_B}\"\n",
        "\n",
        "kern_A: JITKernel = _tmp_A\n",
        "kern_B: JITKernel = _tmp_B\n",
        "\n",
        "prim_A = kern_A.prim_func\n",
        "prim_B = kern_B.prim_func\n",
        "\n",
        "print('A kernel ok:', isinstance(kern_A, JITKernel))\n",
        "print('B kernel ok:', isinstance(kern_B, JITKernel))\n",
        "print('A prim type:', type(prim_A))\n",
        "print('B prim type:', type(prim_B))\n",
        "\n",
        "pre_A = prim_A.script()\n",
        "pre_B = prim_B.script()\n",
        "(OUT_DIR / 'pre_default.py').write_text(pre_A)\n",
        "(OUT_DIR / 'pre_explicit.py').write_text(pre_B)\n",
        "print('Saved pre-lower scripts to', OUT_DIR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4) 通过 `kernel.artifact.device_mod.script()` 拿到 lowered 设备端 IR，并保存到 `artifacts/lower_*.py`，用于观察实际生成的 barrier/TMA/WGMMA 等指令结构"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved lowered device IR to /home/chenxi/tilelang/notebooks/artifacts\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def kernel_impl_kernel(K_desc: T.handle(\"uint8x128\", \"grid_constant\"), Output_desc: T.handle(\"uint8x128\", \"grid_constant\"), Q_desc: T.handle(\"uint8x128\", \"grid_constant\"), V_desc: T.handle(\"uint8x128\", \"grid_constant\")):\n",
            "        T.func_attr({\"calling_conv\": 2, \"dyn_shared_memory_buf\": 40960, \"target\": T.target({\"arch\": \"sm_90\", \"keys\": [\"cuda\", \"gpu\"], \"kind\": \"cuda\", \"max_num_threads\": 1024, \"tag\": \"\", \"thread_warp_size\": 32}), \"thread_extent\": {\"blockIdx.x\": 4, \"blockIdx.y\": 1, \"blockIdx.z\": 1, \"threadIdx.x\": 256, \"threadIdx.y\": 1, \"threadIdx.z\": 1}, \"tir.is_global_func\": T.bool(True), \"tir.kernel_launch_params\": [\"blockIdx.x\", \"blockIdx.y\", \"blockIdx.z\", \"threadIdx.x\", \"th\n",
            "---\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main_kernel(K_desc: T.handle(\"uint8x128\", \"grid_constant\"), Output_desc: T.handle(\"uint8x128\", \"grid_constant\"), Q_desc: T.handle(\"uint8x128\", \"grid_constant\"), V_desc: T.handle(\"uint8x128\", \"grid_constant\")):\n",
            "        T.func_attr({\"calling_conv\": 2, \"dyn_shared_memory_buf\": 40960, \"target\": T.target({\"arch\": \"sm_90\", \"keys\": [\"cuda\", \"gpu\"], \"kind\": \"cuda\", \"max_num_threads\": 1024, \"tag\": \"\", \"thread_warp_size\": 32}), \"thread_extent\": {\"blockIdx.x\": 4, \"blockIdx.y\": 1, \"blockIdx.z\": 1, \"threadIdx.x\": 256, \"threadIdx.y\": 1, \"threadIdx.z\": 1}, \"tir.is_global_func\": T.bool(True), \"tir.kernel_launch_params\": [\"blockIdx.x\", \"blockIdx.y\", \"blockIdx.z\", \"threadIdx.x\", \"threadIdx\n"
          ]
        }
      ],
      "source": [
        "assert isinstance(kern_A, JITKernel) and isinstance(kern_B, JITKernel), \"请先构建 kern_A/kern_B\"\n",
        "\n",
        "def get_device_mod(kernel: JITKernel):\n",
        "    mod = getattr(kernel.artifact, 'device_mod', None)\n",
        "    if mod is None:\n",
        "        mod = getattr(kernel.adapter, 'device_mod', None)\n",
        "    assert mod is not None, \"device_mod 获取失败：请重跑上方构建单元或改用不同 shape 触发重新编译\"\n",
        "    return mod\n",
        "\n",
        "low_A_mod = get_device_mod(kern_A)\n",
        "low_B_mod = get_device_mod(kern_B)\n",
        "\n",
        "low_A = low_A_mod.script()\n",
        "low_B = low_B_mod.script()\n",
        "(OUT_DIR / 'lower_default.py').write_text(low_A)\n",
        "(OUT_DIR / 'lower_explicit.py').write_text(low_B)\n",
        "print('Saved lowered device IR to', OUT_DIR)\n",
        "print(low_A[:800]); print('---'); print(low_B[:800])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5) 基于关键词（mbarrier/tma/wgmma 等）从 lowered IR 提取上下文片段，快速定位流水线/同步相关代码"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Default (lowered) key sections ===\n",
            "        by = T.launch_thread(\"blockIdx.y\", 1)\n",
            "        bz = T.launch_thread(\"blockIdx.z\", 1)\n",
            "        tx = T.launch_thread(\"threadIdx.x\", 256)\n",
            "        T.create_barriers(9)\n",
            "        if T.tl_shuffle_elect(0):\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "\n",
            "        tx = T.launch_thread(\"threadIdx.x\", 256)\n",
            "        T.create_barriers(9)\n",
            "        if T.tl_shuffle_elect(0):\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", V_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Output_desc)\n",
            "\n",
            "        T.create_barriers(9)\n",
            "        if T.tl_shuffle_elect(0):\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", V_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Output_desc)\n",
            "            T.ptx_init_barrier_thread_count(T.get_mbarrier(0), 1)\n",
            "\n",
            "        if T.tl_shuffle_elect(0):\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", V_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Output_desc)\n",
            "            T.ptx_init_barrier_thread_count(T.get_mbarrier(0), 1)\n",
            "            T.ptx_init_barrier_thread_count(T.get_mbarrier(1), 1)\n",
            "\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", V_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Output_desc)\n",
            "            T.p\n",
            "\n",
            "=== Explicit (lowered) key sections ===\n",
            "        by = T.launch_thread(\"blockIdx.y\", 1)\n",
            "        bz = T.launch_thread(\"blockIdx.z\", 1)\n",
            "        tx = T.launch_thread(\"threadIdx.x\", 256)\n",
            "        T.create_barriers(9)\n",
            "        if T.tl_shuffle_elect(0):\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "\n",
            "        tx = T.launch_thread(\"threadIdx.x\", 256)\n",
            "        T.create_barriers(9)\n",
            "        if T.tl_shuffle_elect(0):\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", V_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Output_desc)\n",
            "\n",
            "        T.create_barriers(9)\n",
            "        if T.tl_shuffle_elect(0):\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", V_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Output_desc)\n",
            "            T.ptx_init_barrier_thread_count(T.get_mbarrier(0), 1)\n",
            "\n",
            "        if T.tl_shuffle_elect(0):\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", V_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Output_desc)\n",
            "            T.ptx_init_barrier_thread_count(T.get_mbarrier(0), 1)\n",
            "            T.ptx_init_barrier_thread_count(T.get_mbarrier(1), 1)\n",
            "\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", V_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Output_desc)\n",
            "            T.p\n"
          ]
        }
      ],
      "source": [
        "# Extract and highlight pipeline/barrier-related lines\n",
        "\n",
        "def grep_keywords(text: str, kws):\n",
        "    lines = text.splitlines()\n",
        "    hits = []\n",
        "    for i,l in enumerate(lines):\n",
        "        if any(k in l for k in kws):\n",
        "            start = max(0, i-3)\n",
        "            end = min(len(lines), i+4)\n",
        "            hits.append('\\n'.join(lines[start:end]))\n",
        "    return '\\n\\n'.join(hits)\n",
        "\n",
        "KWs = [\n",
        "    'mbarrier', 'barrier', 'cp.async', 'tma', 'descriptor', 'async', 'wait_group', 'commit_group',\n",
        "    'tlx.async_descriptor_load', 'ldmatrix', 'WGMMA', 'wgmma', 'wmma', 'warpgroup'\n",
        "]\n",
        "\n",
        "print('=== Default (lowered) key sections ===')\n",
        "print(grep_keywords(low_A, KWs)[:2000])\n",
        "print('\\n=== Explicit (lowered) key sections ===')\n",
        "print(grep_keywords(low_B, KWs)[:2000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipelined logger ready. Use install_pipelined_logger()/restore_pipelined() and PipelineLogScope.\n"
          ]
        }
      ],
      "source": [
        "# 安装/恢复 Pipelined 记录器（monkeypatch）\n",
        "import threading, time, json, csv\n",
        "import tilelang.language as TL\n",
        "\n",
        "_PIPELINED_ORIG = getattr(TL, 'Pipelined')\n",
        "_PIPELINE_LOG = []\n",
        "_PIPELINE_CTX = threading.local()\n",
        "\n",
        "# 提供上下文配置，便于在记录里带出当前 config\n",
        "class PipelineLogScope:\n",
        "    def __init__(self, tag: str = None, **cfg):\n",
        "        self.tag = tag\n",
        "        self.cfg = cfg\n",
        "    def __enter__(self):\n",
        "        setattr(_PIPELINE_CTX, 'cfg', self.cfg)\n",
        "        setattr(_PIPELINE_CTX, 'tag', self.tag)\n",
        "    def __exit__(self, exc_type, exc, tb):\n",
        "        setattr(_PIPELINE_CTX, 'cfg', None)\n",
        "        setattr(_PIPELINE_CTX, 'tag', None)\n",
        "\n",
        "\n",
        "def _pipelined_logger(*args, **kwargs):\n",
        "    # 记录关键参数\n",
        "    rec = {\n",
        "        'ts': time.time(),\n",
        "        'tag': getattr(_PIPELINE_CTX, 'tag', None),\n",
        "        'cfg': getattr(_PIPELINE_CTX, 'cfg', None),\n",
        "        'num_stages': kwargs.get('num_stages'),\n",
        "        'order': kwargs.get('order'),\n",
        "        'stage': kwargs.get('stage'),\n",
        "        'group': kwargs.get('group'),\n",
        "        'sync': kwargs.get('sync'),\n",
        "    }\n",
        "    _PIPELINE_LOG.append(rec)\n",
        "    # 调用原始 Pipelined\n",
        "    return _PIPELINED_ORIG(*args, **kwargs)\n",
        "\n",
        "\n",
        "def install_pipelined_logger():\n",
        "    TL.Pipelined = _pipelined_logger\n",
        "    return True\n",
        "\n",
        "\n",
        "def restore_pipelined():\n",
        "    TL.Pipelined = _PIPELINED_ORIG\n",
        "    return True\n",
        "\n",
        "print('Pipelined logger ready. Use install_pipelined_logger()/restore_pipelined() and PipelineLogScope.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-09 16:36:45,342 WARNING:Tunable parameters ['block_M', 'block_N', 'num_stages', 'threads'] already provided during auto-tuning. Skipping compilation and using direct JIT\n",
            "2025-09-09 16:36:45  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[3]`\n",
            "2025-09-09 16:36:55  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
            "2025-09-09 16:36:55,399 WARNING:Tunable parameters ['block_M', 'block_N', 'num_stages', 'threads'] already provided during auto-tuning. Skipping compilation and using direct JIT\n",
            "2025-09-09 16:36:55  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[3]`\n",
            "2025-09-09 16:37:05  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
            "2025-09-09 16:37:05,451 WARNING:Tunable parameters ['block_M', 'block_N', 'num_stages', 'threads'] already provided during auto-tuning. Skipping compilation and using direct JIT\n",
            "2025-09-09 16:37:05  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[3]`\n",
            "2025-09-09 16:37:17  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
            "2025-09-09 16:37:17,045 WARNING:Tunable parameters ['block_M', 'block_N', 'num_stages', 'threads'] already provided during auto-tuning. Skipping compilation and using direct JIT\n",
            "2025-09-09 16:37:17  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `kernel_impl` with `out_idx=[3]`\n",
            "2025-09-09 16:37:26  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `kernel_impl`\n",
            "2025-09-09 16:37:26,998 WARNING:Tunable parameters ['block_M', 'block_N', 'num_stages', 'threads'] already provided during auto-tuning. Skipping compilation and using direct JIT\n",
            "2025-09-09 16:37:27  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `kernel_impl` with `out_idx=[3]`\n",
            "2025-09-09 16:37:38  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `kernel_impl`\n",
            "Total logged calls (cumulative): 5\n",
            "Last 5 samples:\n",
            "{'ts': 1757407005.358359, 'tag': 'explicit-shape-0', 'cfg': {'block_M': 64, 'block_N': 64, 'num_stages': 2, 'threads': 128}, 'num_stages': 2, 'order': [-1, 0, 3, 1, -1, 2], 'stage': [-1, 0, 0, 1, -1, 1], 'group': [[0], [1, 2], [3, 4, 5, 6, 7, 8, 9, 10], [11], [12], [13]], 'sync': None}\n",
            "{'ts': 1757407015.4151464, 'tag': 'explicit-shape-1', 'cfg': {'block_M': 64, 'block_N': 64, 'num_stages': 2, 'threads': 128}, 'num_stages': 2, 'order': [-1, 0, 3, 1, -1, 2], 'stage': [-1, 0, 0, 1, -1, 1], 'group': [[0], [1, 2], [3, 4, 5, 6, 7, 8, 9, 10], [11], [12], [13]], 'sync': None}\n",
            "{'ts': 1757407025.4659069, 'tag': 'explicit-shape-2', 'cfg': {'block_M': 128, 'block_N': 128, 'num_stages': 2, 'threads': 256}, 'num_stages': 2, 'order': [-1, 0, 3, 1, -1, 2], 'stage': [-1, 0, 0, 1, -1, 1], 'group': [[0], [1, 2], [3, 4, 5, 6, 7, 8, 9, 10], [11], [12], [13]], 'sync': None}\n",
            "{'ts': 1757407037.0602388, 'tag': 'default-shape-0', 'cfg': {'block_M': 64, 'block_N': 64, 'num_stages': 1, 'threads': 128}, 'num_stages': 1, 'order': None, 'stage': None, 'group': None, 'sync': None}\n",
            "{'ts': 1757407047.0133867, 'tag': 'default-shape-1', 'cfg': {'block_M': 128, 'block_N': 128, 'num_stages': 2, 'threads': 128}, 'num_stages': 2, 'order': None, 'stage': None, 'group': None, 'sync': None}\n"
          ]
        }
      ],
      "source": [
        "install_pipelined_logger()\n",
        "\n",
        "from tilelang.jit.kernel import JITKernel\n",
        "\n",
        "# 通过改变 seq_len / dim 来避开 JIT 的内存缓存键\n",
        "explicit_runs = [\n",
        "    {'block_M':64,'block_N':64,'num_stages':2,'threads':128, 'batch':1,'heads':1,'seq_len':192,'dim':64},\n",
        "    {'block_M':64,'block_N':64,'num_stages':2,'threads':128, 'batch':1,'heads':1,'seq_len':320,'dim':64},\n",
        "    {'block_M':128,'block_N':128,'num_stages':2,'threads':256,'batch':1,'heads':1,'seq_len':256,'dim':96},\n",
        "]\n",
        "\n",
        "for i, cfg in enumerate(explicit_runs):\n",
        "    tag = f'explicit-shape-{i}'\n",
        "    call_cfg = {k:cfg[k] for k in ['block_M','block_N','num_stages','threads']}\n",
        "    with PipelineLogScope(tag=tag, **call_cfg):\n",
        "        kern = B.flashattn(**cfg, is_causal=False)\n",
        "        assert isinstance(kern, JITKernel)\n",
        "\n",
        "restore_pipelined()\n",
        "\n",
        "print('Total logged calls (cumulative):', len(_PIPELINE_LOG))\n",
        "print('Last 5 samples:')\n",
        "for r in _PIPELINE_LOG[-5:]:\n",
        "    print(r)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tilelang",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
