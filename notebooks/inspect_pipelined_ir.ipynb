{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "问题：默认流水线的阶段结构在 lowered IR 中只体现为 barrier/TMA/MMA 的相对顺序，不会还原为 `order/stage/group` 数组；且我们批量推断的摘要粒度过粗，难以区分不同配置。\n",
        "\n",
        "方案：对 `tilelang.language.pipeline.Pipelined` 做“可恢复的 monkeypatch”记录器，采集每一次构图时传入的关键字参数（`num_stages/order/stage/group/sync`），连同当前配置（block_M/N、threads 等）一起记录。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipelined logger ready. Use install_pipelined_logger()/restore_pipelined() and PipelineLogScope.\n"
          ]
        }
      ],
      "source": [
        "# 安装/恢复 Pipelined 记录器（monkeypatch）\n",
        "import threading, time, json, csv\n",
        "import tilelang.language as TL\n",
        "\n",
        "_PIPELINED_ORIG = getattr(TL, 'Pipelined')\n",
        "_PIPELINE_LOG = []\n",
        "_PIPELINE_CTX = threading.local()\n",
        "\n",
        "# 提供上下文配置，便于在记录里带出当前 config\n",
        "class PipelineLogScope:\n",
        "    def __init__(self, tag: str = None, **cfg):\n",
        "        self.tag = tag\n",
        "        self.cfg = cfg\n",
        "    def __enter__(self):\n",
        "        setattr(_PIPELINE_CTX, 'cfg', self.cfg)\n",
        "        setattr(_PIPELINE_CTX, 'tag', self.tag)\n",
        "    def __exit__(self, exc_type, exc, tb):\n",
        "        setattr(_PIPELINE_CTX, 'cfg', None)\n",
        "        setattr(_PIPELINE_CTX, 'tag', None)\n",
        "\n",
        "\n",
        "def _pipelined_logger(*args, **kwargs):\n",
        "    # 记录关键参数\n",
        "    rec = {\n",
        "        'ts': time.time(),\n",
        "        'tag': getattr(_PIPELINE_CTX, 'tag', None),\n",
        "        'cfg': getattr(_PIPELINE_CTX, 'cfg', None),\n",
        "        'num_stages': kwargs.get('num_stages'),\n",
        "        'order': kwargs.get('order'),\n",
        "        'stage': kwargs.get('stage'),\n",
        "        'group': kwargs.get('group'),\n",
        "        'sync': kwargs.get('sync'),\n",
        "    }\n",
        "    _PIPELINE_LOG.append(rec)\n",
        "    # 调用原始 Pipelined\n",
        "    return _PIPELINED_ORIG(*args, **kwargs)\n",
        "\n",
        "\n",
        "def install_pipelined_logger():\n",
        "    TL.Pipelined = _pipelined_logger\n",
        "    return True\n",
        "\n",
        "\n",
        "def restore_pipelined():\n",
        "    TL.Pipelined = _PIPELINED_ORIG\n",
        "    return True\n",
        "\n",
        "print('Pipelined logger ready. Use install_pipelined_logger()/restore_pipelined() and PipelineLogScope.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-09 16:25:43,473 WARNING:Found kernel in memory cache. For better performance, consider using `@tilelang.autotune` instead of direct AutoTuner.from_kernel.\n",
            "Logged calls: 0\n",
            "Sample (first 3):\n"
          ]
        }
      ],
      "source": [
        "install_pipelined_logger()\n",
        "\n",
        "from tilelang.jit.kernel import JITKernel\n",
        "explicit_cfgs = [\n",
        "    {'block_M':64,'block_N':64,'num_stages':2,'threads':128},\n",
        "    {'block_M':128,'block_N':128,'num_stages':2,'threads':256},\n",
        "]\n",
        "\n",
        "for i,cfg in enumerate(explicit_cfgs):\n",
        "    with PipelineLogScope(tag=f'explicit-{i}', **cfg):\n",
        "        kern = B.flashattn(batch=1, heads=1, seq_len=256, dim=64, is_causal=False, **cfg)\n",
        "        assert isinstance(kern, JITKernel)\n",
        "\n",
        "default_cfgs = [\n",
        "    {'block_M':64,'block_N':64,'num_stages':1,'threads':128},\n",
        "    {'block_M':128,'block_N':128,'num_stages':2,'threads':128},\n",
        "]\n",
        "\n",
        "for i,cfg in enumerate(default_cfgs):\n",
        "    with PipelineLogScope(tag=f'default-{i}', **cfg):\n",
        "        kern = A.flashattn(batch=1, heads=1, seq_len=256, dim=64, is_causal=False, **cfg)\n",
        "        assert isinstance(kern, JITKernel)\n",
        "\n",
        "restore_pipelined()\n",
        "\n",
        "# 导出日志\n",
        "log_json = OUT_DIR / 'pipeline_tune_calls.json'\n",
        "log_csv = OUT_DIR / 'pipeline_tune_calls.csv'\n",
        "\n",
        "with open(log_json, 'w') as f:\n",
        "    json.dump(_PIPELINE_LOG, f, indent=2)\n",
        "\n",
        "if _PIPELINE_LOG:\n",
        "    keys = sorted(set().union(*[set(x.keys()) for x in _PIPELINE_LOG]))\n",
        "    with open(log_csv, 'w', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=keys)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(_PIPELINE_LOG)\n",
        "\n",
        "print('Logged calls:', len(_PIPELINE_LOG))\n",
        "print('Sample (first 3):')\n",
        "for r in _PIPELINE_LOG[:3]:\n",
        "    print(r)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-09 16:25:43,485 WARNING:Tunable parameters ['block_M', 'block_N', 'num_stages', 'threads'] already provided during auto-tuning. Skipping compilation and using direct JIT\n",
            "2025-09-09 16:25:43  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[3]`\n",
            "2025-09-09 16:25:53  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
            "2025-09-09 16:25:53,615 WARNING:Tunable parameters ['block_M', 'block_N', 'num_stages', 'threads'] already provided during auto-tuning. Skipping compilation and using direct JIT\n",
            "2025-09-09 16:25:53  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[3]`\n",
            "2025-09-09 16:26:03  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
            "2025-09-09 16:26:03,752 WARNING:Tunable parameters ['block_M', 'block_N', 'num_stages', 'threads'] already provided during auto-tuning. Skipping compilation and using direct JIT\n",
            "2025-09-09 16:26:03  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `main` with `out_idx=[3]`\n",
            "2025-09-09 16:26:15  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `main`\n",
            "2025-09-09 16:26:15,355 WARNING:Tunable parameters ['block_M', 'block_N', 'num_stages', 'threads'] already provided during auto-tuning. Skipping compilation and using direct JIT\n",
            "2025-09-09 16:26:15  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `kernel_impl` with `out_idx=[3]`\n",
            "2025-09-09 16:26:25  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `kernel_impl`\n",
            "2025-09-09 16:26:25,361 WARNING:Tunable parameters ['block_M', 'block_N', 'num_stages', 'threads'] already provided during auto-tuning. Skipping compilation and using direct JIT\n",
            "2025-09-09 16:26:25  [TileLang:tilelang.jit.kernel:INFO]: TileLang begins to compile kernel `kernel_impl` with `out_idx=[3]`\n",
            "2025-09-09 16:26:36  [TileLang:tilelang.jit.kernel:INFO]: TileLang completes to compile kernel `kernel_impl`\n",
            "Total logged calls (cumulative): 5\n",
            "Last 5 samples:\n",
            "{'ts': 1757406343.5005949, 'tag': 'explicit-shape-0', 'cfg': {'block_M': 64, 'block_N': 64, 'num_stages': 2, 'threads': 128}, 'num_stages': 2, 'order': [-1, 0, 3, 1, -1, 2], 'stage': [-1, 0, 0, 1, -1, 1], 'group': [[0], [1, 2], [3, 4, 5, 6, 7, 8, 9, 10], [11], [12], [13]], 'sync': None}\n",
            "{'ts': 1757406353.6299481, 'tag': 'explicit-shape-1', 'cfg': {'block_M': 64, 'block_N': 64, 'num_stages': 2, 'threads': 128}, 'num_stages': 2, 'order': [-1, 0, 3, 1, -1, 2], 'stage': [-1, 0, 0, 1, -1, 1], 'group': [[0], [1, 2], [3, 4, 5, 6, 7, 8, 9, 10], [11], [12], [13]], 'sync': None}\n",
            "{'ts': 1757406363.7666466, 'tag': 'explicit-shape-2', 'cfg': {'block_M': 128, 'block_N': 128, 'num_stages': 2, 'threads': 256}, 'num_stages': 2, 'order': [-1, 0, 3, 1, -1, 2], 'stage': [-1, 0, 0, 1, -1, 1], 'group': [[0], [1, 2], [3, 4, 5, 6, 7, 8, 9, 10], [11], [12], [13]], 'sync': None}\n",
            "{'ts': 1757406375.369646, 'tag': 'default-shape-0', 'cfg': {'block_M': 64, 'block_N': 64, 'num_stages': 1, 'threads': 128}, 'num_stages': 1, 'order': None, 'stage': None, 'group': None, 'sync': None}\n",
            "{'ts': 1757406385.376614, 'tag': 'default-shape-1', 'cfg': {'block_M': 128, 'block_N': 128, 'num_stages': 2, 'threads': 128}, 'num_stages': 2, 'order': None, 'stage': None, 'group': None, 'sync': None}\n"
          ]
        }
      ],
      "source": [
        "# 扩大量：多样化 shape 以强制重新构图，多次记录显式/默认的实际调用\n",
        "install_pipelined_logger()\n",
        "\n",
        "from tilelang.jit.kernel import JITKernel\n",
        "\n",
        "# 通过改变 seq_len / dim 来避开 JIT 的内存缓存键\n",
        "explicit_runs = [\n",
        "    {'block_M':64,'block_N':64,'num_stages':2,'threads':128, 'batch':1,'heads':1,'seq_len':192,'dim':64},\n",
        "    {'block_M':64,'block_N':64,'num_stages':2,'threads':128, 'batch':1,'heads':1,'seq_len':320,'dim':64},\n",
        "    {'block_M':128,'block_N':128,'num_stages':2,'threads':256,'batch':1,'heads':1,'seq_len':256,'dim':96},\n",
        "]\n",
        "\n",
        "for i, cfg in enumerate(explicit_runs):\n",
        "    tag = f'explicit-shape-{i}'\n",
        "    call_cfg = {k:cfg[k] for k in ['block_M','block_N','num_stages','threads']}\n",
        "    with PipelineLogScope(tag=tag, **call_cfg):\n",
        "        kern = B.flashattn(**cfg, is_causal=False)\n",
        "        assert isinstance(kern, JITKernel)\n",
        "\n",
        "# 默认版同理\n",
        "default_runs = [\n",
        "    {'block_M':64,'block_N':64,'num_stages':1,'threads':128, 'batch':1,'heads':1,'seq_len':192,'dim':64},\n",
        "    {'block_M':128,'block_N':128,'num_stages':2,'threads':128,'batch':1,'heads':1,'seq_len':320,'dim':64},\n",
        "]\n",
        "\n",
        "for i, cfg in enumerate(default_runs):\n",
        "    tag = f'default-shape-{i}'\n",
        "    call_cfg = {k:cfg[k] for k in ['block_M','block_N','num_stages','threads']}\n",
        "    with PipelineLogScope(tag=tag, **call_cfg):\n",
        "        kern = A.flashattn(**cfg, is_causal=False)\n",
        "        assert isinstance(kern, JITKernel)\n",
        "\n",
        "restore_pipelined()\n",
        "\n",
        "print('Total logged calls (cumulative):', len(_PIPELINE_LOG))\n",
        "print('Last 5 samples:')\n",
        "for r in _PIPELINE_LOG[-5:]:\n",
        "    print(r)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1) 环境初始化与路径/版本打印"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TileLang version: 0.1.5\n",
            "Using tilelang from: /home/chenxi/miniconda3/envs/tilelang/lib/python3.12/site-packages/tilelang/__init__.py\n",
            "A: /home/chenxi/tilelang/examples/flash_attention/example_mha_fwd_bshd.py\n",
            "B: /home/chenxi/tilelang/examples/flash_attention/example_mha_fwd_bshd_wgmma_pipelined-2.py\n"
          ]
        }
      ],
      "source": [
        "# Inspect TileLang Pipelined IR: default vs explicit order/stage/group\n",
        "import os, sys, importlib.util, textwrap, re\n",
        "from pathlib import Path\n",
        "import tilelang\n",
        "from tvm import tir\n",
        "\n",
        "ROOT = Path('/home/chenxi')\n",
        "A_PATH = ROOT / 'tilelang/examples/flash_attention/example_mha_fwd_bshd.py'\n",
        "B_PATH = ROOT / 'tilelang/examples/flash_attention/example_mha_fwd_bshd_wgmma_pipelined-2.py'\n",
        "OUT_DIR = ROOT / 'tilelang/notebooks/artifacts'\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('TileLang version:', tilelang.__version__)\n",
        "print('Using tilelang from:', tilelang.__file__)\n",
        "print('A:', A_PATH)\n",
        "print('B:', B_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2) 用 `importlib` 的 `spec_from_file_location` 安全加载两个示例模块，并打印加载的模块名"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Joint配置数量: 6\n",
            "Tile配置数量: 4 (固定stage=1)\n",
            "Loaded modules: example_mha_fwd_bshd example_mha_fwd_bshd_wgmma_pipelined_2\n"
          ]
        }
      ],
      "source": [
        "# Helpers to safely import example modules without triggering autotune side-effects\n",
        "\n",
        "def load_module(path: str, mod_name: str):\n",
        "    spec = importlib.util.spec_from_file_location(mod_name, path)\n",
        "    mod = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(mod)\n",
        "    return mod\n",
        "\n",
        "A = load_module(str(A_PATH), 'example_mha_fwd_bshd')\n",
        "B = load_module(str(B_PATH), 'example_mha_fwd_bshd_wgmma_pipelined_2')\n",
        "\n",
        "print('Loaded modules:', A.__name__, B.__name__)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3) 分别调用两份脚本的 `flashattn(...)` 得到 `JITKernel`，从中取 `prim_func`，即编译前的 TIR ，保存，用于直观看到 `T.Pipelined(...)` 的源级调用位置与上下文"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-09 16:26:36,626 WARNING:Found kernel in memory cache. For better performance, consider using `@tilelang.autotune` instead of direct AutoTuner.from_kernel.\n",
            "2025-09-09 16:26:36,629 WARNING:Found kernel in memory cache. For better performance, consider using `@tilelang.autotune` instead of direct AutoTuner.from_kernel.\n",
            "A kernel ok: True\n",
            "B kernel ok: True\n",
            "A prim type: <class 'tvm.tir.function.PrimFunc'>\n",
            "B prim type: <class 'tvm.tir.function.PrimFunc'>\n",
            "Saved pre-lower scripts to /home/chenxi/tilelang/notebooks/artifacts\n"
          ]
        }
      ],
      "source": [
        "# Build kernels via JIT (avoid manual lower), then extract PrimFuncs\n",
        "from tilelang.jit.kernel import JITKernel\n",
        "\n",
        "# A: use flashattn (decorated), returns JITKernel; autotune will be bypassed if all tunables provided\n",
        "_tmp_A = A.flashattn(batch=1, heads=1, seq_len=256, dim=64, is_causal=False,\n",
        "                     block_M=64, block_N=64, num_stages=2, threads=128)\n",
        "# B: explicit pipelined version\n",
        "_tmp_B = B.flashattn(batch=1, heads=1, seq_len=256, dim=64, is_causal=False,\n",
        "                     block_M=64, block_N=64, num_stages=2, threads=128)\n",
        "\n",
        "assert isinstance(_tmp_A, JITKernel), f\"A should be JITKernel, got {_tmp_A}\"\n",
        "assert isinstance(_tmp_B, JITKernel), f\"B should be JITKernel, got {_tmp_B}\"\n",
        "\n",
        "kern_A: JITKernel = _tmp_A\n",
        "kern_B: JITKernel = _tmp_B\n",
        "\n",
        "prim_A = kern_A.prim_func\n",
        "prim_B = kern_B.prim_func\n",
        "\n",
        "print('A kernel ok:', isinstance(kern_A, JITKernel))\n",
        "print('B kernel ok:', isinstance(kern_B, JITKernel))\n",
        "print('A prim type:', type(prim_A))\n",
        "print('B prim type:', type(prim_B))\n",
        "\n",
        "pre_A = prim_A.script()\n",
        "pre_B = prim_B.script()\n",
        "(OUT_DIR / 'pre_default.py').write_text(pre_A)\n",
        "(OUT_DIR / 'pre_explicit.py').write_text(pre_B)\n",
        "print('Saved pre-lower scripts to', OUT_DIR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4) 通过 `kernel.artifact.device_mod.script()` 拿到 lowered 设备端 IR，并保存到 `artifacts/lower_*.py`，用于观察实际生成的 barrier/TMA/WGMMA 等指令结构"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved lowered device IR to /home/chenxi/tilelang/notebooks/artifacts\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def kernel_impl_kernel(K_desc: T.handle(\"uint8x128\", \"grid_constant\"), Output_desc: T.handle(\"uint8x128\", \"grid_constant\"), Q_desc: T.handle(\"uint8x128\", \"grid_constant\"), V_desc: T.handle(\"uint8x128\", \"grid_constant\")):\n",
            "        T.func_attr({\"calling_conv\": 2, \"dyn_shared_memory_buf\": 40960, \"target\": T.target({\"arch\": \"sm_90\", \"keys\": [\"cuda\", \"gpu\"], \"kind\": \"cuda\", \"max_num_threads\": 1024, \"tag\": \"\", \"thread_warp_size\": 32}), \"thread_extent\": {\"blockIdx.x\": 4, \"blockIdx.y\": 1, \"blockIdx.z\": 1, \"threadIdx.x\": 256, \"threadIdx.y\": 1, \"threadIdx.z\": 1}, \"tir.is_global_func\": T.bool(True), \"tir.kernel_launch_params\": [\"blockIdx.x\", \"blockIdx.y\", \"blockIdx.z\", \"threadIdx.x\", \"th\n",
            "---\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main_kernel(K_desc: T.handle(\"uint8x128\", \"grid_constant\"), Output_desc: T.handle(\"uint8x128\", \"grid_constant\"), Q_desc: T.handle(\"uint8x128\", \"grid_constant\"), V_desc: T.handle(\"uint8x128\", \"grid_constant\")):\n",
            "        T.func_attr({\"calling_conv\": 2, \"dyn_shared_memory_buf\": 40960, \"target\": T.target({\"arch\": \"sm_90\", \"keys\": [\"cuda\", \"gpu\"], \"kind\": \"cuda\", \"max_num_threads\": 1024, \"tag\": \"\", \"thread_warp_size\": 32}), \"thread_extent\": {\"blockIdx.x\": 4, \"blockIdx.y\": 1, \"blockIdx.z\": 1, \"threadIdx.x\": 256, \"threadIdx.y\": 1, \"threadIdx.z\": 1}, \"tir.is_global_func\": T.bool(True), \"tir.kernel_launch_params\": [\"blockIdx.x\", \"blockIdx.y\", \"blockIdx.z\", \"threadIdx.x\", \"threadIdx\n"
          ]
        }
      ],
      "source": [
        "# Lower to device IR from kernels (use compiled artifacts)\n",
        "low_A = kern_A.artifact.device_mod.script()\n",
        "low_B = kern_B.artifact.device_mod.script()\n",
        "(OUT_DIR / 'lower_default.py').write_text(low_A)\n",
        "(OUT_DIR / 'lower_explicit.py').write_text(low_B)\n",
        "print('Saved lowered device IR to', OUT_DIR)\n",
        "print(low_A[:800])\n",
        "print('---')\n",
        "print(low_B[:800])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5) 基于关键词（mbarrier/tma/wgmma 等）从 lowered IR 提取上下文片段，快速定位流水线/同步相关代码"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Default (lowered) key sections ===\n",
            "        by = T.launch_thread(\"blockIdx.y\", 1)\n",
            "        bz = T.launch_thread(\"blockIdx.z\", 1)\n",
            "        tx = T.launch_thread(\"threadIdx.x\", 256)\n",
            "        T.create_barriers(9)\n",
            "        if T.tl_shuffle_elect(0):\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "\n",
            "        tx = T.launch_thread(\"threadIdx.x\", 256)\n",
            "        T.create_barriers(9)\n",
            "        if T.tl_shuffle_elect(0):\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", V_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Output_desc)\n",
            "\n",
            "        T.create_barriers(9)\n",
            "        if T.tl_shuffle_elect(0):\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", V_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Output_desc)\n",
            "            T.ptx_init_barrier_thread_count(T.get_mbarrier(0), 1)\n",
            "\n",
            "        if T.tl_shuffle_elect(0):\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", V_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Output_desc)\n",
            "            T.ptx_init_barrier_thread_count(T.get_mbarrier(0), 1)\n",
            "            T.ptx_init_barrier_thread_count(T.get_mbarrier(1), 1)\n",
            "\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", V_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Output_desc)\n",
            "            T.p\n",
            "\n",
            "=== Explicit (lowered) key sections ===\n",
            "        by = T.launch_thread(\"blockIdx.y\", 1)\n",
            "        bz = T.launch_thread(\"blockIdx.z\", 1)\n",
            "        tx = T.launch_thread(\"threadIdx.x\", 256)\n",
            "        T.create_barriers(9)\n",
            "        if T.tl_shuffle_elect(0):\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "\n",
            "        tx = T.launch_thread(\"threadIdx.x\", 256)\n",
            "        T.create_barriers(9)\n",
            "        if T.tl_shuffle_elect(0):\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", V_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Output_desc)\n",
            "\n",
            "        T.create_barriers(9)\n",
            "        if T.tl_shuffle_elect(0):\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", V_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Output_desc)\n",
            "            T.ptx_init_barrier_thread_count(T.get_mbarrier(0), 1)\n",
            "\n",
            "        if T.tl_shuffle_elect(0):\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", V_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Output_desc)\n",
            "            T.ptx_init_barrier_thread_count(T.get_mbarrier(0), 1)\n",
            "            T.ptx_init_barrier_thread_count(T.get_mbarrier(1), 1)\n",
            "\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", V_desc)\n",
            "            T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Output_desc)\n",
            "            T.p\n"
          ]
        }
      ],
      "source": [
        "# Extract and highlight pipeline/barrier-related lines\n",
        "\n",
        "def grep_keywords(text: str, kws):\n",
        "    lines = text.splitlines()\n",
        "    hits = []\n",
        "    for i,l in enumerate(lines):\n",
        "        if any(k in l for k in kws):\n",
        "            start = max(0, i-3)\n",
        "            end = min(len(lines), i+4)\n",
        "            hits.append('\\n'.join(lines[start:end]))\n",
        "    return '\\n\\n'.join(hits)\n",
        "\n",
        "KWs = [\n",
        "    'mbarrier', 'barrier', 'cp.async', 'tma', 'descriptor', 'async', 'wait_group', 'commit_group',\n",
        "    'tlx.async_descriptor_load', 'ldmatrix', 'WGMMA', 'wgmma', 'wmma', 'warpgroup'\n",
        "]\n",
        "\n",
        "print('=== Default (lowered) key sections ===')\n",
        "print(grep_keywords(low_A, KWs)[:2000])\n",
        "print('\\n=== Explicit (lowered) key sections ===')\n",
        "print(grep_keywords(low_B, KWs)[:2000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Default inferred phase order (coarse): ['BARRIER->TMA', 'BARRIER->TMA']\n",
            "Explicit inferred phase order (coarse): ['BARRIER->TMA', 'BARRIER', 'BARRIER->TMA']\n",
            "\n",
            "Default first 2 phases (context):\n",
            "  Phase 0:\n",
            "    [00037] BARRIER: T.create_barriers(9)\n",
            "    [00039] TMA: T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "    [00040] TMA: T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "    [00041] TMA: T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", V_desc)\n",
            "    [00042] TMA: T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Output_desc)\n",
            "    [00043] BARRIER: T.ptx_init_barrier_thread_count(T.get_mbarrier(0), 1)\n",
            "    [00044] BARRIER: T.ptx_init_barrier_thread_count(T.get_mbarrier(1), 1)\n",
            "    [00045] BARRIER: T.ptx_init_barrier_thread_count(T.get_mbarrier(2), 1)\n",
            "  Phase 1:\n",
            "    [00111] BARRIER: T.mbarrier_wait_parity(T.get_mbarrier(k % 2 + 2), k // 2)\n",
            "    [00113] BARRIER: T.ptx_arrive_barrier(T.get_mbarrier(k % 2 + 6), 0, tx == 0)\n",
            "    [00118] TMA: T.ptx_stmatrix(0, 4, T.tvm_access_ptr(T.type_annotation(\"float16\"), buf_dyn_shmem, 16384 + (tx // 32 * 1024 + tx % 16 * 64 + i * 16 + tx % 32 // 16 * 8), 8, 2), T.pack_b16(T.Cast(\"float16\", acc_o_1[i * 8]), T.Cast(\"float16\", acc_o_1[i * 8 + 1])), T.pack_b16(T.Cast(\"float16\", acc_o_1[i * 8 + 2]), T.Cast(\"float16\", acc_o_1[i * 8 + 3])), T.pack_b16(T.Cast(\"float16\", acc_o_1[i * 8 + 4]), T.Cast(\"float16\", acc_o_1[i * 8 + 5])), T.pack_b16(T.Cast(\"float16\", acc_o_1[i * 8 + 6]), T.Cast(\"float16\", acc_o_1[i * 8 + 7])))\n",
            "    [00122] TMA: T.tma_store(Output_desc, T.tvm_access_ptr(T.type_annotation(\"float16\"), buf_dyn_shmem, 16384, 4096, 1), 0, 0, bx * 64, 0, 0)\n",
            "    [00123] TMA: T.tma_store_arrive()\n",
            "    [00124] TMA: T.tma_store_wait()\n",
            "\n",
            "Explicit first 2 phases (context):\n",
            "  Phase 0:\n",
            "    [00037] BARRIER: T.create_barriers(9)\n",
            "    [00039] TMA: T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Q_desc)\n",
            "    [00040] TMA: T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", K_desc)\n",
            "    [00041] TMA: T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", V_desc)\n",
            "    [00042] TMA: T.call_extern(\"handle\", \"tl::prefetch_tma_descriptor\", Output_desc)\n",
            "    [00043] BARRIER: T.ptx_init_barrier_thread_count(T.get_mbarrier(0), 1)\n",
            "    [00044] BARRIER: T.ptx_init_barrier_thread_count(T.get_mbarrier(1), 1)\n",
            "    [00045] BARRIER: T.ptx_init_barrier_thread_count(T.get_mbarrier(2), 1)\n",
            "  Phase 1:\n",
            "    [00111] BARRIER: T.mbarrier_wait_parity(T.get_mbarrier((k + 1) % 2), (k + 1) // 2)\n",
            "    [00113] BARRIER: T.ptx_arrive_barrier(T.get_mbarrier((k + 1) % 2 + 4), 0, tx == 0)\n",
            "    [00117] BARRIER: T.mbarrier_wait_parity(T.get_mbarrier(k % 2 + 2), k // 2)\n",
            "    [00119] BARRIER: T.ptx_arrive_barrier(T.get_mbarrier(k % 2 + 6), 0, tx == 0)\n"
          ]
        }
      ],
      "source": [
        "# 针对默认流水线：基于 lowered IR 的启发式推断阶段化（统计每个循环步的TMA/Barrier/WGMMA顺序）\n",
        "import re\n",
        "\n",
        "def analyze_pipeline_order(lowered_text: str, max_lines: int = 10000):\n",
        "    lines = lowered_text.splitlines()[:max_lines]\n",
        "    # 以 mbarrier 初始化/arrive/wait 或者 prefetch_tma_descriptor 作为“段落”切分点\n",
        "    markers = ['prefetch_tma_descriptor', 'ptx_init_barrier_thread_count', 'mbarrier', 'wgmma', 'wmma']\n",
        "    events = []\n",
        "    for i,l in enumerate(lines):\n",
        "        tag = None\n",
        "        if 'prefetch_tma_descriptor' in l or 'async_descriptor_load' in l or 'tma' in l:\n",
        "            tag = 'TMA'\n",
        "        elif 'ptx_init_barrier_thread_count' in l or 'create_barriers' in l or 'mbarrier' in l:\n",
        "            tag = 'BARRIER'\n",
        "        elif 'wgmma' in l.lower() or 'wmma' in l.lower():\n",
        "            tag = 'MMA'\n",
        "        if tag:\n",
        "            events.append((i, tag, l.strip()))\n",
        "    # 把邻近的事件聚成“阶段”，并统计顺序\n",
        "    phases = []\n",
        "    cur = []\n",
        "    last_idx = -999\n",
        "    for idx, tag, txt in events:\n",
        "        if idx - last_idx > 20 and cur:\n",
        "            phases.append(cur)\n",
        "            cur = []\n",
        "        cur.append((idx, tag, txt))\n",
        "        last_idx = idx\n",
        "    if cur:\n",
        "        phases.append(cur)\n",
        "    # 阶段摘要\n",
        "    summary = []\n",
        "    for p in phases:\n",
        "        tags = [t for _, t, _ in p]\n",
        "        summary.append('->'.join(sorted(set(tags), key=tags.index)))\n",
        "    return phases, summary\n",
        "\n",
        "phases_A, summary_A = analyze_pipeline_order(low_A)\n",
        "phases_B, summary_B = analyze_pipeline_order(low_B)\n",
        "\n",
        "print('Default inferred phase order (coarse):', summary_A[:12])\n",
        "print('Explicit inferred phase order (coarse):', summary_B[:12])\n",
        "\n",
        "# 可选：打印前几个阶段的上下文\n",
        "for name, phases in [('Default', phases_A), ('Explicit', phases_B)]:\n",
        "    print(f\"\\n{name} first 2 phases (context):\")\n",
        "    for pi, phase in enumerate(phases[:2]):\n",
        "        print(f\"  Phase {pi}:\")\n",
        "        for idx, tag, txt in phase[:8]:\n",
        "            print(f\"    [{idx:05d}] {tag}: {txt}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7) 默认流水线批量推断与导出\n",
        "\n",
        "    - 遍历一组 `block_M/block_N/num_stages` 组合，构建 JITKernel 并获取 lowered IR。\n",
        "    - 用前面的 `analyze_pipeline_order` 提取每个配置的阶段摘要（coarse order）。\n",
        "    - 导出为 `artifacts/pipeline_inferred_default.json` 与 `artifacts/pipeline_inferred_default.csv`，便于与显式版对比。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-09 16:26:36,667 WARNING:Found kernel in memory cache. For better performance, consider using `@tilelang.autotune` instead of direct AutoTuner.from_kernel.\n",
            "2025-09-09 16:26:36,671 WARNING:Found kernel in memory cache. For better performance, consider using `@tilelang.autotune` instead of direct AutoTuner.from_kernel.\n",
            "2025-09-09 16:26:36,674 WARNING:Found kernel in memory cache. For better performance, consider using `@tilelang.autotune` instead of direct AutoTuner.from_kernel.\n",
            "2025-09-09 16:26:36,678 WARNING:Found kernel in memory cache. For better performance, consider using `@tilelang.autotune` instead of direct AutoTuner.from_kernel.\n",
            "2025-09-09 16:26:36,682 WARNING:Found kernel in memory cache. For better performance, consider using `@tilelang.autotune` instead of direct AutoTuner.from_kernel.\n",
            "2025-09-09 16:26:36,686 WARNING:Found kernel in memory cache. For better performance, consider using `@tilelang.autotune` instead of direct AutoTuner.from_kernel.\n",
            "2025-09-09 16:26:36,689 WARNING:Found kernel in memory cache. For better performance, consider using `@tilelang.autotune` instead of direct AutoTuner.from_kernel.\n",
            "Exported 8 inferred entries:\n",
            " - /home/chenxi/tilelang/notebooks/artifacts/pipeline_inferred_default.json\n",
            " - /home/chenxi/tilelang/notebooks/artifacts/pipeline_inferred_default.csv\n",
            "Preview:\n",
            "{'block_M': 64, 'block_N': 64, 'num_stages': 1, 'threads': 128, 'summary': 'BARRIER->TMA | BARRIER->TMA'}\n",
            "{'block_M': 64, 'block_N': 64, 'num_stages': 2, 'threads': 128, 'summary': 'BARRIER->TMA | BARRIER->TMA'}\n",
            "{'block_M': 64, 'block_N': 128, 'num_stages': 1, 'threads': 128, 'summary': 'BARRIER->TMA | BARRIER->TMA'}\n",
            "{'block_M': 64, 'block_N': 128, 'num_stages': 2, 'threads': 128, 'summary': 'BARRIER->TMA | BARRIER->TMA'}\n",
            "{'block_M': 128, 'block_N': 64, 'num_stages': 1, 'threads': 128, 'summary': 'BARRIER->TMA | BARRIER->TMA'}\n"
          ]
        }
      ],
      "source": [
        "# 批量推断默认流水线阶段摘要并导出\n",
        "import itertools, json, csv\n",
        "from tilelang.jit.kernel import JITKernel\n",
        "\n",
        "# 可根据需要调整搜索空间（保持小范围，避免编译时间过长）\n",
        "BLOCK_M = [64, 128]\n",
        "BLOCK_N = [64, 128]\n",
        "NUM_STAGES = [1, 2]\n",
        "THREADS = [128]\n",
        "\n",
        "combos = list(itertools.product(BLOCK_M, BLOCK_N, NUM_STAGES, THREADS))\n",
        "\n",
        "results = []\n",
        "for bm, bn, ns, th in combos:\n",
        "    try:\n",
        "        # 构建默认流水线内核（不显式 order/stage/group）\n",
        "        kern = A.flashattn(batch=1, heads=1, seq_len=256, dim=64, is_causal=False,\n",
        "                           block_M=bm, block_N=bn, num_stages=ns, threads=th)\n",
        "        assert isinstance(kern, JITKernel)\n",
        "        low = kern.artifact.device_mod.script()\n",
        "        # 提取阶段摘要\n",
        "        _, summary = analyze_pipeline_order(low)\n",
        "        results.append({\n",
        "            'block_M': bm,\n",
        "            'block_N': bn,\n",
        "            'num_stages': ns,\n",
        "            'threads': th,\n",
        "            'summary': ' | '.join(summary[:8])  # 截断显示，避免太长\n",
        "        })\n",
        "    except Exception as e:\n",
        "        results.append({\n",
        "            'block_M': bm,\n",
        "            'block_N': bn,\n",
        "            'num_stages': ns,\n",
        "            'threads': th,\n",
        "            'summary': f'ERROR: {type(e).__name__}: {e}'\n",
        "        })\n",
        "\n",
        "json_out = OUT_DIR / 'pipeline_inferred_default.json'\n",
        "csv_out = OUT_DIR / 'pipeline_inferred_default.csv'\n",
        "\n",
        "with open(json_out, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "with open(csv_out, 'w', newline='') as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=list(results[0].keys()))\n",
        "    writer.writeheader()\n",
        "    writer.writerows(results)\n",
        "\n",
        "print(f'Exported {len(results)} inferred entries:')\n",
        "print(' -', json_out)\n",
        "print(' -', csv_out)\n",
        "print('Preview:')\n",
        "for r in results[:5]:\n",
        "    print(r)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PrimFunc script中未匹配到参数，回退到源码解析...\n",
            "Parsed from: Source\n",
            "order = [-1, 0, 3, 1, -1, 2]\n",
            "stage = [-1, 0, 0, 1, -1, 1]\n",
            "group = [[0]\n",
            "sync  = None\n",
            "...context snippet...\n",
            "\n",
            "                    loop_range,\n",
            "                    num_stages=num_stages,\n",
            "                    order=[-1, 0, 3, 1, -1, 2],\n",
            "                    stage=[-1, 0, 0, 1, -1, 1],\n",
            "                    group=[[0], [1, 2], [3, 4, 5, 6, 7, 8, 9, 10], [11], [12], [13]]\n"
          ]
        }
      ],
      "source": [
        "# 更健壮的 T.Pipelined(...) 参数提取实现\n",
        "import re, ast\n",
        "\n",
        "def extract_pipelined_args(text: str):\n",
        "    # 找到 T.Pipelined( 的起点，然后做括号配对拿到完整参数字符串\n",
        "    start = text.find('T.Pipelined(')\n",
        "    if start == -1:\n",
        "        return None\n",
        "    i = start + len('T.Pipelined(')\n",
        "    depth = 1\n",
        "    args = []\n",
        "    while i < len(text) and depth > 0:\n",
        "        ch = text[i]\n",
        "        args.append(ch)\n",
        "        if ch == '(':\n",
        "            depth += 1\n",
        "        elif ch == ')':\n",
        "            depth -= 1\n",
        "        i += 1\n",
        "    arg_str = ''.join(args[:-1])  # 去掉最后一个闭括号\n",
        "    def grab(key):\n",
        "        m = re.search(rf\"{key}\\s*=\\s*(\\[[^\\]]*\\])\", arg_str)\n",
        "        if not m:\n",
        "            return None\n",
        "        try:\n",
        "            return ast.literal_eval(m.group(1))\n",
        "        except Exception:\n",
        "            return m.group(1)\n",
        "    return {\n",
        "        'order': grab('order'),\n",
        "        'stage': grab('stage'),\n",
        "        'group': grab('group'),\n",
        "        'sync': grab('sync'),\n",
        "        'arg_str': arg_str\n",
        "    }\n",
        "\n",
        "res = extract_pipelined_args(prim_B.script())\n",
        "if res is None or (res['order'] is None and res['stage'] is None and res['group'] is None):\n",
        "    print('PrimFunc script中未匹配到参数，回退到源码解析...')\n",
        "    src = B_PATH.read_text()\n",
        "    res = extract_pipelined_args(src)\n",
        "\n",
        "print('Parsed from:', 'PrimFunc' if 'PrimFunc' in locals() else 'Source')\n",
        "print('order =', res.get('order'))\n",
        "print('stage =', res.get('stage'))\n",
        "print('group =', res.get('group'))\n",
        "print('sync  =', res.get('sync'))\n",
        "if res and res.get('arg_str'):\n",
        "    print('...context snippet...')\n",
        "    print(res['arg_str'][:400])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 方法A：遍历 get_configs()，抓取每个配置的 order/stage/group/num_stages 并导出\n",
        "\n",
        "- 对 `example_mha_fwd_bshd_wgmma_pipelined-2.py`：`order/stage/group` 是在源码里显式指定的，因此不同 config 会得到相同的三元组，但我们依旧记录每个 config 的形状（block_M/N、threads、num_stages）方便关联。\n",
        "- 不进行 benchmark，只做构图与脚本解析，速度快。\n",
        "- 导出为 `artifacts/pipeline_orders.json` 与 `artifacts/pipeline_orders.csv`。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exported 36 entries to:\n",
            " - /home/chenxi/tilelang/notebooks/artifacts/pipeline_orders.json\n",
            " - /home/chenxi/tilelang/notebooks/artifacts/pipeline_orders.csv\n",
            "Preview (first 3):\n",
            "{'block_M': 64, 'block_N': 64, 'threads': 128, 'num_stages_cfg': 1, 'order': [-1, 0, 3, 1, -1, 2], 'stage': [-1, 0, 0, 1, -1, 1], 'group': [[0], [1, 2], [3, 4, 5, 6, 7, 8, 9, 10], [11], [12], [13]], 'sync': None, 'num_stages_kw': 'num_stages'}\n",
            "{'block_M': 64, 'block_N': 64, 'threads': 128, 'num_stages_cfg': 2, 'order': [-1, 0, 3, 1, -1, 2], 'stage': [-1, 0, 0, 1, -1, 1], 'group': [[0], [1, 2], [3, 4, 5, 6, 7, 8, 9, 10], [11], [12], [13]], 'sync': None, 'num_stages_kw': 'num_stages'}\n",
            "{'block_M': 64, 'block_N': 64, 'threads': 128, 'num_stages_cfg': 3, 'order': [-1, 0, 3, 1, -1, 2], 'stage': [-1, 0, 0, 1, -1, 1], 'group': [[0], [1, 2], [3, 4, 5, 6, 7, 8, 9, 10], [11], [12], [13]], 'sync': None, 'num_stages_kw': 'num_stages'}\n"
          ]
        }
      ],
      "source": [
        "# 遍历 get_configs() 并导出（方法A）\n",
        "import json, csv\n",
        "\n",
        "# 读取 -2.py 中的 configs 生成器（若无该函数则回退到简单集）\n",
        "get_cfg = getattr(B, 'get_configs', None)\n",
        "if get_cfg is None:\n",
        "    cfgs = [{'block_M': 128, 'block_N': 128, 'num_stages': 2, 'threads': 256}]\n",
        "else:\n",
        "    cfgs = get_cfg()\n",
        "\n",
        "records = []\n",
        "\n",
        "# 复用前面已实现的 AST 解析器（优先脚本，回退源码）\n",
        "from typing import Any, Dict\n",
        "\n",
        "def get_pipelined_kwargs_from_script_or_source() -> Dict[str, Any]:\n",
        "    cands = extract_all_pipelined_kwargs(prim_B.script())\n",
        "    if not cands:\n",
        "        cands = extract_all_pipelined_kwargs(B_PATH.read_text())\n",
        "    # 取包含最多关键字段的那个\n",
        "    best = None\n",
        "    score = -1\n",
        "    for seg, kw in cands:\n",
        "        s = sum(1 for k in ('order','stage','group','sync','num_stages') if k in kw)\n",
        "        if s > score:\n",
        "            best, score = (seg, kw), s\n",
        "    return best[1] if best else {}\n",
        "\n",
        "kw_fixed = get_pipelined_kwargs_from_script_or_source()\n",
        "\n",
        "for cfg in cfgs:\n",
        "    entry = {\n",
        "        'block_M': cfg.get('block_M'),\n",
        "        'block_N': cfg.get('block_N'),\n",
        "        'threads': cfg.get('threads'),\n",
        "        'num_stages_cfg': cfg.get('num_stages'),\n",
        "        # 来自脚本/源码的显式参数：\n",
        "        'order': kw_fixed.get('order'),\n",
        "        'stage': kw_fixed.get('stage'),\n",
        "        'group': kw_fixed.get('group'),\n",
        "        'sync': kw_fixed.get('sync'),\n",
        "        'num_stages_kw': kw_fixed.get('num_stages'),\n",
        "    }\n",
        "    records.append(entry)\n",
        "\n",
        "json_path = OUT_DIR / 'pipeline_orders.json'\n",
        "csv_path = OUT_DIR / 'pipeline_orders.csv'\n",
        "\n",
        "with open(json_path, 'w') as f:\n",
        "    json.dump(records, f, indent=2)\n",
        "\n",
        "with open(csv_path, 'w', newline='') as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=list(records[0].keys()))\n",
        "    writer.writeheader()\n",
        "    writer.writerows(records)\n",
        "\n",
        "print(f'Exported {len(records)} entries to:')\n",
        "print(' -', json_path)\n",
        "print(' -', csv_path)\n",
        "print('Preview (first 3):')\n",
        "for r in records[:3]:\n",
        "    print(r)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matched kwargs keys: ['group', 'num_stages', 'order', 'stage']\n",
            "order = [-1, 0, 3, 1, -1, 2]\n",
            "stage = [-1, 0, 0, 1, -1, 1]\n",
            "group = [[0], [1, 2], [3, 4, 5, 6, 7, 8, 9, 10], [11], [12], [13]]\n",
            "sync  = None\n",
            "num_stages = num_stages\n"
          ]
        }
      ],
      "source": [
        "# 使用 AST 解析 T.Pipelined(...)，支持嵌套列表（group）与多处匹配\n",
        "import ast\n",
        "\n",
        "def find_all_pipelined_segments(text: str):\n",
        "    segs = []\n",
        "    i = 0\n",
        "    while True:\n",
        "        start = text.find('T.Pipelined(', i)\n",
        "        if start == -1:\n",
        "            break\n",
        "        j = start + len('T.Pipelined(')\n",
        "        depth = 1\n",
        "        args = []\n",
        "        while j < len(text) and depth > 0:\n",
        "            ch = text[j]\n",
        "            args.append(ch)\n",
        "            if ch == '(':\n",
        "                depth += 1\n",
        "            elif ch == ')':\n",
        "                depth -= 1\n",
        "            j += 1\n",
        "        seg = ''.join(args[:-1])\n",
        "        segs.append(seg)\n",
        "        i = j\n",
        "    return segs\n",
        "\n",
        "\n",
        "def parse_kwargs_by_ast(arg_str: str):\n",
        "    # 构造一个可被 ast 解析的表达式：F(<arg_str>)\n",
        "    expr = ast.parse('F(' + arg_str + ')', mode='eval')\n",
        "    call = expr.body\n",
        "    assert isinstance(call, ast.Call)\n",
        "    out = {}\n",
        "    for kw in call.keywords:\n",
        "        key = kw.arg\n",
        "        try:\n",
        "            val = ast.literal_eval(kw.value)\n",
        "        except Exception:\n",
        "            # 退而求其次，用源码片段\n",
        "            val = ast.get_source_segment('F(' + arg_str + ')', kw.value)\n",
        "        out[key] = val\n",
        "    return out\n",
        "\n",
        "\n",
        "def extract_all_pipelined_kwargs(text: str):\n",
        "    results = []\n",
        "    for seg in find_all_pipelined_segments(text):\n",
        "        try:\n",
        "            kw = parse_kwargs_by_ast(seg)\n",
        "            results.append((seg, kw))\n",
        "        except Exception:\n",
        "            continue\n",
        "    return results\n",
        "\n",
        "# 先从 PrimFunc script 中解析，找包含 order/stage/group 的那一个；否则回退源码\n",
        "candidates = extract_all_pipelined_kwargs(prim_B.script())\n",
        "if not candidates:\n",
        "    candidates = extract_all_pipelined_kwargs(B_PATH.read_text())\n",
        "\n",
        "best = None\n",
        "score = -1\n",
        "for seg, kw in candidates:\n",
        "    s = 0\n",
        "    for k in ('order','stage','group','sync','num_stages'):\n",
        "        if k in kw:\n",
        "            s += 1\n",
        "    if s > score:\n",
        "        best, score = (seg, kw), s\n",
        "\n",
        "if best is None:\n",
        "    print('未能解析到任何 T.Pipelined(...) 关键字参数。')\n",
        "else:\n",
        "    seg, kw = best\n",
        "    print('Matched kwargs keys:', sorted(list(kw.keys())))\n",
        "    print('order =', kw.get('order'))\n",
        "    print('stage =', kw.get('stage'))\n",
        "    print('group =', kw.get('group'))\n",
        "    print('sync  =', kw.get('sync'))\n",
        "    print('num_stages =', kw.get('num_stages'))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tilelang",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
